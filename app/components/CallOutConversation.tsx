// @ts-nocheck
'use client';

import { useEffect, useRef, useState } from 'react';
import { getAIResponse } from '@/lib/gpt/getAIResponse';
import { speakWithElevenLabs } from '@/lib/voice/speakWithElevenLabs';
import ManualInputBox from './ManualInputBox';
import MicSelector from './MicSelector';

declare global {
  interface Window {
    webkitSpeechRecognition: any;
    SpeechRecognition: any;
  }
  interface SpeechRecognitionEvent extends Event {
    results: SpeechRecognitionResultList;
  }
}

export default function CallOutConversation() {
  const [conversation, setConversation] = useState<string[]>([]);
  const [isActive, setIsActive] = useState(false);
  const recognitionRef = useRef<any>(null);
  const isSpeakingRef = useRef(false);

  const createNewRecognition = () => {
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    const recognition = new SpeechRecognition();
    recognition.lang = 'zh-CN';
    recognition.interimResults = false;
    recognition.continuous = true;
    return recognition;
  };

  const restartRecognition = () => {
    const newRecog = createNewRecognition();

    newRecog.onresult = async (event: SpeechRecognitionEvent) => {
      if (isSpeakingRef.current || !isActive) return;
      const userText = event.results[event.results.length - 1][0].transcript.trim();
      if (!userText) return;

      setConversation((prev) => [...prev, `ğŸ§‘ åŒ»ç”Ÿ: ${userText}`]);

      const prompt = `
ä½ æ˜¯ Lucyï¼Œè¯·ç›´æ¥ç”¨è‡ªç„¶ç®€æ´çš„è‹±æ–‡å›ç­”åŒ»ç”Ÿåˆšæ‰è¯´çš„å†…å®¹ï¼Œä¸è¦è§£é‡Šä½ æ˜¯è°ï¼Œä¹Ÿä¸è¦æåŠAIæˆ–ç§˜ä¹¦ã€‚
åŒ»ç”Ÿåˆšæ‰è¯´: ${userText}
`;

      const reply = await getAIResponse(prompt);
      setConversation((prev) => [...prev, `ğŸ¤– Lucy: ${reply}`]);

      if (recognitionRef.current) recognitionRef.current.stop();
      isSpeakingRef.current = true;

      try {
        await speakWithElevenLabs(reply);
      } catch (err) {
        console.error('è¯­éŸ³æ’­æŠ¥å¤±è´¥:', err);
      } finally {
        isSpeakingRef.current = false;
        restartRecognition();
        recognitionRef.current.start();
      }
    };

    newRecog.onerror = (event: any) => console.error('è¯†åˆ«å‡ºé”™:', event.error);
    newRecog.onend = () => {
      if (isActive && !isSpeakingRef.current) {
        try {
          newRecog.start();
        } catch (err) {
          console.error('é‡å¯å¤±è´¥:', err);
        }
      }
    };

    recognitionRef.current = newRecog;
  };

  useEffect(() => {
    if (isActive) {
      restartRecognition();
      recognitionRef.current?.start();
    } else {
      recognitionRef.current?.stop();
    }
  }, [isActive]);

  return (
    <div className="space-y-4">
      <h2 className="text-lg font-semibold text-purple-700">ğŸ“ æ‹¨æ‰“ç”µè¯æ¨¡å¼</h2>

      <MicSelector
        className="mb-2"
        onSelected={(id) => {
          // åªè®°å½•ï¼Œä¸æ”¹å˜åŸæµç¨‹
          // æœªæ¥å¦‚æœç”¨ getUserMedia å½•éŸ³ï¼ˆé WebSpeechï¼‰ï¼Œè¿™é‡Œå¯ä»¥æ¥å…¥ deviceId
          console.log("Preferred mic deviceId:", id);
        }}
      />

      <button
        onClick={() => setIsActive((v) => !v)}
        className={`px-4 py-2 rounded text-white ${isActive ? 'bg-red-600' : 'bg-green-600'}`}
      >
        {isActive ? 'åœæ­¢é€šè¯' : 'å¼€å§‹é€šè¯'}
      </button>

      <div className="bg-gray-100 p-4 rounded h-64 overflow-y-auto space-y-1 text-sm">
        {conversation.map((line, idx) => (
          <div key={idx}>{line}</div>
        ))}
      </div>

      <ManualInputBox
        onSend={(text) => {
          setConversation((prev) => [...prev, `ğŸ§‘ Lucyï¼ˆæ‰‹åŠ¨ï¼‰ï¼š${text}`]);
        }}
      />
    </div>
  );
}
